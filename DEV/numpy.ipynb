{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5209ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array((1, 2, 3, 4, 5))\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75aa9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array(42)\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b70382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52241956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138415f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b4eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = np.array(42)\n",
    "b = np.array([1, 2, 3, 4, 5])\n",
    "c = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "d = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n",
    "\n",
    "print(a.ndim)\n",
    "print(b.ndim)\n",
    "print(c.ndim)\n",
    "print(d.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7176a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[1 2 3 4]]]]]\n",
      "number of dimensions : 5\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4], ndmin=5)\n",
    "\n",
    "print(arr)\n",
    "print('number of dimensions :', arr.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07726b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba7ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(arr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542ab0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "\n",
    "print(arr[2] + arr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5223e458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd element on 1st row:  2\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,2,3,4,5], [6,7,8,9,10]])\n",
    "\n",
    "print('2nd element on 1st row: ', arr[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfaa1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cars  passings\n",
      "0    BMW         3\n",
      "1  Volvo         7\n",
      "2   Ford         2\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "mydataset = {\n",
    "  'cars': [\"BMW\", \"Volvo\", \"Ford\"],\n",
    "  'passings': [3, 7, 2]\n",
    "}\n",
    "\n",
    "myvar = pandas.DataFrame(mydataset)\n",
    "\n",
    "print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015c5d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    7\n",
      "2    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = [1, 7, 2]\n",
    "\n",
    "myvar = pd.Series(a)\n",
    "\n",
    "print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c8a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day1    420\n",
      "day2    380\n",
      "day3    390\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "calories = {\"day1\": 420, \"day2\": 380, \"day3\": 390}\n",
    "\n",
    "myvar = pd.Series(calories)\n",
    "\n",
    "print(myvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m     spark.stop()\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    103\u001b[39m create_tables(mysql_connection)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Insert data into the tables\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43minsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmysql_connection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomer_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Close the connection\u001b[39;00m\n\u001b[32m    107\u001b[39m mysql_connection.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36minsert_data\u001b[39m\u001b[34m(connection, customer_df, branch_df)\u001b[39m\n\u001b[32m     75\u001b[39m cursor = connection.cursor()\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Insert data into CDW_SAPP_CUSTOMER\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcustomer_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     79\u001b[39m         sql = \u001b[33m\"\u001b[39m\u001b[33mINSERT INTO CDW_SAPP_CUSTOMER (CUST_ID, FNAME, MNAME, LNAME, STREET, CITY, STATE, ZIP, PHONE, EMAIL, GENDER, DOB) VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m         values = (row[\u001b[33m\"\u001b[39m\u001b[33mcust_id\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mfname\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mmname\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mlname\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mstreet\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mphone\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33memail\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mdob\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlton.powell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \n\u001b[32m   1245\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlton.powell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlton.powell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "import mysql.connector\n",
    "import os  # Import the os module\n",
    "import socket # Import socket\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardCapstoneETL\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/path/to/mysql-connector-java.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the absolute paths to the JSON files\n",
    "branch_file_path = \"cdw_sapp_branch.json\"\n",
    "customer_file_path = \"cdw_sapp_customer.json\"\n",
    "credit_file_path = \"cdw_sapp_credit.json\"\n",
    "\n",
    "branch_absolute_path = os.path.abspath(branch_file_path)\n",
    "customer_absolute_path = os.path.abspath(customer_file_path)\n",
    "credit_absolute_path = os.path.abspath(credit_file_path)\n",
    "\n",
    "\n",
    "# Read the JSON files into DataFrames\n",
    "try:\n",
    "    branch_df = spark.read.json(branch_absolute_path)\n",
    "    customer_df = spark.read.json(customer_absolute_path)\n",
    "    credit_df = spark.read.json(credit_absolute_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Define MySQL connection parameters\n",
    "host = \"127.0.0.1\"  # Replace with your MySQL host.  Fixed the unterminated string and removed the port.\n",
    "user = \"root\"  # Replace with your MySQL user\n",
    "password = \"password\"  # Replace with your MySQL password\n",
    "database = \"creditcard_capstone\"  # Replace with your MySQL database\n",
    "\n",
    "# Function to create a MySQL connection\n",
    "def create_mysql_connection():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=database\n",
    "        )\n",
    "        return connection\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error connecting to MySQL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create tables in MySQL\n",
    "def create_tables(connection):\n",
    "    if connection is None:\n",
    "        return\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS CDW_SAPP_CUSTOMER (CUST_ID INT PRIMARY KEY, FNAME VARCHAR(255), MNAME VARCHAR(255), LNAME VARCHAR(255), STREET VARCHAR(255), CITY VARCHAR(255), STATE VARCHAR(255), ZIP INT, PHONE VARCHAR(20), EMAIL VARCHAR(255), GENDER VARCHAR(10), DOB DATE)\")\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS CDW_SAPP_BRANCH (BRANCH_ID INT PRIMARY KEY, BRANCH_NAME VARCHAR(255), BRANCH_CITY VARCHAR(255), BRANCH_STATE VARCHAR(255))\")\n",
    "        connection.commit()\n",
    "        print(\"Tables created successfully.\")\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "        connection.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Function to insert data into MySQL tables\n",
    "def insert_data(connection, customer_df, branch_df):\n",
    "    if connection is None:\n",
    "        return\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        # Insert data into CDW_SAPP_CUSTOMER\n",
    "        for row in customer_df.collect():\n",
    "            sql = \"INSERT INTO CDW_SAPP_CUSTOMER (CUST_ID, FNAME, MNAME, LNAME, STREET, CITY, STATE, ZIP, PHONE, EMAIL, GENDER, DOB) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "            values = (row[\"cust_id\"], row[\"fname\"], row[\"mname\"], row[\"lname\"], row[\"street\"], row[\"city\"], row[\"state\"], row[\"zip\"], row[\"phone\"], row[\"email\"], row[\"gender\"], row[\"dob\"])\n",
    "            cursor.execute(sql, values)\n",
    "\n",
    "        # Insert data into CDW_SAPP_BRANCH\n",
    "        for row in branch_df.collect():\n",
    "            sql = \"INSERT INTO CDW_SAPP_BRANCH (BRANCH_ID, BRANCH_NAME, BRANCH_CITY, BRANCH_STATE) VALUES (%s, %s, %s, %s)\"\n",
    "            values = (row[\"branch_id\"], row[\"branch_name\"], row[\"branch_city\"], row[\"branch_state\"])\n",
    "            cursor.execute(sql, values)\n",
    "\n",
    "        connection.commit()\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error inserting data: {e}\")\n",
    "        connection.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    # Create a MySQL connection\n",
    "    mysql_connection = create_mysql_connection()\n",
    "    if mysql_connection:\n",
    "        # Create tables in MySQL\n",
    "        create_tables(mysql_connection)\n",
    "        # Insert data into the tables\n",
    "        insert_data(mysql_connection, customer_df, branch_df)\n",
    "        # Close the connection\n",
    "        mysql_connection.close()\n",
    "        print(\"MySQL connection closed.\")\n",
    "    else:\n",
    "        print(\"Failed to establish MySQL connection.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
